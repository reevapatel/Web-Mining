{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE9WGE_m-4J-"
      },
      "source": [
        "# HW 4: Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKVEJOmf-4KC"
      },
      "source": [
        "## Q1: Extract data using regular expression (2 points)\n",
        "Suppose you have scraped the text shown below from an online source (https://www.google.com/finance/). \n",
        "Define a `extract` function which:\n",
        "- takes a piece of text (in the format of shown below) as an input\n",
        "- extracts data into a DataFrame with columns 'Ticker','Name','Article','Media','Time','Price',and 'Change' using regular expression\n",
        "- returns the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"all\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JflQsleeXRoy",
        "outputId": "dff67e55-03ca-4182-c29a-c2e925759a01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-19T07:27:54.060105Z",
          "start_time": "2021-10-19T07:27:50.568579Z"
        },
        "id": "Cq9Xzhnw-4KD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import normalize\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB1zoGaSIQ08"
      },
      "outputs": [],
      "source": [
        "text = '''QQQ\n",
        "Invesco QQQ Trust Series 1\n",
        "Invesco Expands QQQ Innovation Suite to Include Small-Cap ETF\n",
        "PR Newswire • 4 hours ago\n",
        "$265.62\n",
        "1.13%\n",
        "add_circle_outline\n",
        "AAPL\n",
        "Apple Inc\n",
        "Estimating The Fair Value Of Apple Inc. (NASDAQ:AAPL)\n",
        "Yahoo Finance • 4 hours ago\n",
        "$140.41\n",
        "1.50%\n",
        "add_circle_outline\n",
        "TSLA\n",
        "Tesla Inc\n",
        "Could This Tesla Stock Unbalanced Iron Condor Return 23%?\n",
        "Investor's Business Daily • 1 hour ago\n",
        "$218.30\n",
        "0.49%\n",
        "add_circle_outline\n",
        "AMZN\n",
        "Amazon.com, Inc.\n",
        "The Regulators of Facebook, Google and Amazon Also Invest in the Companies' Stocks\n",
        "Wall Street Journal • 2 days ago\n",
        "$110.91\n",
        "1.76%\n",
        "add_circle_outline'''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-19T07:27:54.082105Z",
          "start_time": "2021-10-19T07:27:54.072106Z"
        },
        "id": "StFfIbLB-4KG"
      },
      "outputs": [],
      "source": [
        "def extract(text):\n",
        "   \n",
        "    #add your codes\n",
        "  \n",
        "    l = (text.split('\\n'))\n",
        "  \n",
        "    text1 = (re.sub('•', '\\n', text))\n",
        "    l = (text1.split('add_circle_outline'))\n",
        "    l1 = [i.strip().split('\\n') for i in l if (len(i)>0)]\n",
        "    \n",
        "\n",
        "    result = pd.DataFrame(l1,columns=['Ticker','Name','Article','Media','Time','Price', 'Change'])\n",
        "\n",
        "  \n",
        "  \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extract(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "1YW-WDLjJdml",
        "outputId": "c99cc135-cc7c-4f40-d534-8b889ac1aabc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Ticker                        Name  \\\n",
              "0    QQQ  Invesco QQQ Trust Series 1   \n",
              "1   AAPL                   Apple Inc   \n",
              "2   TSLA                   Tesla Inc   \n",
              "3   AMZN            Amazon.com, Inc.   \n",
              "\n",
              "                                             Article  \\\n",
              "0  Invesco Expands QQQ Innovation Suite to Includ...   \n",
              "1  Estimating The Fair Value Of Apple Inc. (NASDA...   \n",
              "2  Could This Tesla Stock Unbalanced Iron Condor ...   \n",
              "3  The Regulators of Facebook, Google and Amazon ...   \n",
              "\n",
              "                        Media          Time    Price Change  \n",
              "0                PR Newswire    4 hours ago  $265.62  1.13%  \n",
              "1              Yahoo Finance    4 hours ago  $140.41  1.50%  \n",
              "2  Investor's Business Daily     1 hour ago  $218.30  0.49%  \n",
              "3        Wall Street Journal     2 days ago  $110.91  1.76%  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-259fe5e1-7cdb-473a-b840-fd89d896dbea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ticker</th>\n",
              "      <th>Name</th>\n",
              "      <th>Article</th>\n",
              "      <th>Media</th>\n",
              "      <th>Time</th>\n",
              "      <th>Price</th>\n",
              "      <th>Change</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>QQQ</td>\n",
              "      <td>Invesco QQQ Trust Series 1</td>\n",
              "      <td>Invesco Expands QQQ Innovation Suite to Includ...</td>\n",
              "      <td>PR Newswire</td>\n",
              "      <td>4 hours ago</td>\n",
              "      <td>$265.62</td>\n",
              "      <td>1.13%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>Apple Inc</td>\n",
              "      <td>Estimating The Fair Value Of Apple Inc. (NASDA...</td>\n",
              "      <td>Yahoo Finance</td>\n",
              "      <td>4 hours ago</td>\n",
              "      <td>$140.41</td>\n",
              "      <td>1.50%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TSLA</td>\n",
              "      <td>Tesla Inc</td>\n",
              "      <td>Could This Tesla Stock Unbalanced Iron Condor ...</td>\n",
              "      <td>Investor's Business Daily</td>\n",
              "      <td>1 hour ago</td>\n",
              "      <td>$218.30</td>\n",
              "      <td>0.49%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AMZN</td>\n",
              "      <td>Amazon.com, Inc.</td>\n",
              "      <td>The Regulators of Facebook, Google and Amazon ...</td>\n",
              "      <td>Wall Street Journal</td>\n",
              "      <td>2 days ago</td>\n",
              "      <td>$110.91</td>\n",
              "      <td>1.76%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-259fe5e1-7cdb-473a-b840-fd89d896dbea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-259fe5e1-7cdb-473a-b840-fd89d896dbea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-259fe5e1-7cdb-473a-b840-fd89d896dbea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aab-6OWh-4KI"
      },
      "source": [
        "## Q2: Analyze a document (8 points)\n",
        "\n",
        "When you have a long document, you would like to \n",
        "- Quanitfy how concrete a sentence is\n",
        "- Create a concise summary while preserving it's key information content and overall meaning. Let's implement an `extractive method` based on the concept of TF-IDF. The idea is to identify the key sentences from an article and use them as a summary. \n",
        "\n",
        "\n",
        "Carefully follow the following steps to achieve these two targets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsnCv2VZ-4KI"
      },
      "source": [
        "### Q2.1. Preprocess the input document (4 points, each step 0.5 point (see below), overall function and logic 2 points)\n",
        "\n",
        "Define a function `proprocess(doc, lemmatized = True, remove_stopword = True, lower_case = True, remove_punctuation = True, pos_tag = False)` \n",
        "- Inputs with four parameters:\n",
        "    - `doc`: an input string (e.g. a document)\n",
        "    - `lemmatized`: an optional boolean parameter to indicate if tokens are lemmatized. The default value is True (i.e. tokens are lemmatized).\n",
        "    - `remove_stopword`: an optional boolean parameter to remove stop words. The default value is True, i.e., remove stop words. \n",
        "    - `remove_punctuation`: optional boolean parameter to remove punctuations. The default values is True, i.e., remove all punctuations.\n",
        "    - `lower_case`: optional boolean parameter to convert all tokens to lower case. The default option is True, i.e., lowercase all tokens.\n",
        "    - `pos_tag`: optional boolean parameter to add a POS tag for each token. The default option is False, i.e., no POS tagging.  \n",
        "       \n",
        "- Split the input `doc` into sentences. Hint, typically, \"\\n\\n\" is used to separate paragraphs. Make sure each sentence does not cross over two paragraphs. (0.5 point)\n",
        "\n",
        "\n",
        "- Tokenize each sentence into unigram tokens and also process the tokens as follows:\n",
        "    - If `lemmatized` is True, lemmatize all unigrams. (0.5 point)\n",
        "    - If `remove_stopword` is set to True, remove all stop words. (0.5 point)\n",
        "    - If `remove_punctuation` is set to True, remove all punctuations. (0.5 point)\n",
        "    - If `lower_case` is set to True, convert all tokens to lower case (0.5 point)\n",
        "    - If `pos_tag` is set to True, find the POS tag for each token and form a tuple for each token, e.g., ('recently', 'ADV'). Either Penn tags or Universal tags are fine. See mapping of these two tagging systems here: https://universaldependencies.org/tagset-conversion/en-penn-uposf.html\n",
        "\n",
        "- Return the original sentence list (`sents`) and also the tokenized (or tagged) sentence list (`tokenized_sents`). \n",
        "   \n",
        "(Hint: you can use [nltk](https://www.nltk.org/api/nltk.html) and [spacy](https://spacy.io/api/token#attributes) package for this task.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-19T07:27:54.556104Z",
          "start_time": "2021-10-19T07:27:54.096107Z"
        },
        "id": "A9gBjG6V-4KK"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "    \n",
        "def preprocess(doc, lemmatized=True, pos_tag = False, remove_stopword=True, lower_case = True, remove_punctuation = True):\n",
        "    \n",
        "    paragraph = re.sub('\\n\\n', '. ',doc) \n",
        "    sents = sent_tokenize(paragraph)\n",
        "    result = sents\n",
        "    # tokens=[]\n",
        "    # pattern = r'\\w[\\w\\',-]*\\w' \n",
        "    # for i in sents:\n",
        "    #   tokens.append(nltk.regexp_tokenize(i, pattern))\n",
        "    # result = tokens\n",
        "    # print(\"sents: \",result)\n",
        "\n",
        "    temp = []\n",
        "    if(lemmatized == True):\n",
        "      result = []\n",
        "      for i in sents:\n",
        "        doc1 = nlp(i)\n",
        "        result.append([token.lemma_ for token in doc1])\n",
        "      temp = result\n",
        "      # print(\"lemma: \", result)\n",
        " \n",
        "      \n",
        "      \n",
        "    # lemmatizer = WordNetLemmatizer()\n",
        "    # tokens=[]\n",
        "    # pattern = r'\\w[\\w\\',-]*\\w' \n",
        "    # for i in sents:\n",
        "    #   tokens.append(nltk.regexp_tokenize(i, pattern))\n",
        "    # lemmatized_text = [lemmatizer.lemmatize(j) for i in tokens for j in i]\n",
        "    # print(lemmatized_text)\n",
        "\n",
        "    if(remove_stopword==True):\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      result = [[i for i in w if not i.lower() in stop_words] for w in result]\n",
        "      # filtered_sentence = []\n",
        "      # filtered_sentence = [w for w in lemmatized_text if w not in stop_words]\n",
        "      # print(\"stopword: \", result)\n",
        "      \n",
        "\n",
        "    # t = nlp(result)\n",
        "    # if(remove_punctuation==True):\n",
        "    #   temp = filtered_sentence\n",
        "    #   for token in t:\n",
        "    #     if(token.is_punct==True):\n",
        "    #       t.remove(token)\n",
        "    #   print(t)\n",
        "\n",
        "    if(remove_punctuation==True):\n",
        "      words=[]    \n",
        "      result = [[word for word in i if word.isalnum()]  for i in result]\n",
        "      # print(\"punct: \", result)\n",
        "\n",
        "    \n",
        "    if(lower_case==True):\n",
        "       result = [[i.lower() for i in w] for w in result]\n",
        "      #  print(\"lowercase: \", result)\n",
        "       \n",
        "\n",
        "    if(pos_tag==True):\n",
        "      result = []\n",
        "      for w in sents:\n",
        "        result1 = [i.pos_ for i in nlp(w) ] \n",
        "        pos_result = [i.text for i in nlp(w) ] \n",
        "        result.append(list(zip(pos_result,result1)))\n",
        "      # print(\"pos: \",result)\n",
        "    \n",
        "           \n",
        "    return sents, result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-19T07:27:54.566108Z",
          "start_time": "2021-10-19T07:27:54.558107Z"
        },
        "id": "kenx0JgK-4KL"
      },
      "outputs": [],
      "source": [
        "# load test document\n",
        "\n",
        "text = open(\"power_of_nlp.txt\", \"r\", encoding='utf-8').read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-19T07:27:56.835401Z",
          "start_time": "2021-10-19T07:27:54.568105Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drwiHlEb-4KL",
        "outputId": "20453fa1-e68d-47b7-94d6-77fefd5086d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Power of Natural Language Processing. \n",
            " ['power', 'natural', 'language', 'processing'] \n",
            "\n",
            "\n",
            "Until recently, the conventional wisdom was that while AI was better than humans at data-driven decision making tasks, it was still inferior to humans for cognitive and creative ones. \n",
            " ['recently', 'conventional', 'wisdom', 'ai', 'well', 'human', 'data', 'drive', 'decision', 'make', 'task', 'still', 'inferior', 'human', 'cognitive', 'creative', 'one'] \n",
            "\n",
            "\n",
            "But in the past two years language-based AI has advanced by leaps and bounds, changing common notions of what this technology can do.. . \n",
            " ['past', 'two', 'year', 'language', 'base', 'ai', 'advance', 'leap', 'bound', 'change', 'common', 'notion', 'technology'] \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# test with all default options:\n",
        "\n",
        "sents, tokenized_sents = preprocess(text)\n",
        "\n",
        "for i in range(3):\n",
        "    print(sents[i], \"\\n\",tokenized_sents[i],\"\\n\\n\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZnm8rScIQ0_",
        "outputId": "35259c04-ccfd-4f8b-ebfa-f93459d4a747",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Power of Natural Language Processing. \n",
            " [('The', 'DET'), ('Power', 'PROPN'), ('of', 'ADP'), ('Natural', 'PROPN'), ('Language', 'PROPN'), ('Processing', 'PROPN'), ('.', 'PUNCT')] \n",
            "\n",
            "\n",
            "Until recently, the conventional wisdom was that while AI was better than humans at data-driven decision making tasks, it was still inferior to humans for cognitive and creative ones. \n",
            " [('Until', 'ADP'), ('recently', 'ADV'), (',', 'PUNCT'), ('the', 'DET'), ('conventional', 'ADJ'), ('wisdom', 'NOUN'), ('was', 'AUX'), ('that', 'SCONJ'), ('while', 'SCONJ'), ('AI', 'PROPN'), ('was', 'AUX'), ('better', 'ADJ'), ('than', 'ADP'), ('humans', 'NOUN'), ('at', 'ADP'), ('data', 'NOUN'), ('-', 'PUNCT'), ('driven', 'VERB'), ('decision', 'NOUN'), ('making', 'VERB'), ('tasks', 'NOUN'), (',', 'PUNCT'), ('it', 'PRON'), ('was', 'AUX'), ('still', 'ADV'), ('inferior', 'ADJ'), ('to', 'ADP'), ('humans', 'NOUN'), ('for', 'ADP'), ('cognitive', 'ADJ'), ('and', 'CCONJ'), ('creative', 'ADJ'), ('ones', 'NOUN'), ('.', 'PUNCT')] \n",
            "\n",
            "\n",
            "But in the past two years language-based AI has advanced by leaps and bounds, changing common notions of what this technology can do.. . \n",
            " [('But', 'CCONJ'), ('in', 'ADP'), ('the', 'DET'), ('past', 'ADJ'), ('two', 'NUM'), ('years', 'NOUN'), ('language', 'NOUN'), ('-', 'PUNCT'), ('based', 'VERB'), ('AI', 'PROPN'), ('has', 'AUX'), ('advanced', 'VERB'), ('by', 'ADP'), ('leaps', 'NOUN'), ('and', 'CCONJ'), ('bounds', 'NOUN'), (',', 'PUNCT'), ('changing', 'VERB'), ('common', 'ADJ'), ('notions', 'NOUN'), ('of', 'ADP'), ('what', 'PRON'), ('this', 'DET'), ('technology', 'NOUN'), ('can', 'AUX'), ('do', 'AUX'), ('..', 'PUNCT'), ('.', 'PUNCT')] \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# process text without remove stopwords, punctuation, lowercase, but with pos tagging\n",
        "\n",
        "sents, tokenized_sents = preprocess(text, lemmatized = False, pos_tag = True, \n",
        "                                    remove_stopword=False, remove_punctuation = False, \n",
        "                                    lower_case = False)\n",
        "\n",
        "for i in range(3):\n",
        "    print(sents[i], \"\\n\",tokenized_sents[i],\"\\n\\n\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g4iFQ7EIQ1A"
      },
      "source": [
        "### Q2.2. Quantify sentence concreteness\n",
        "\n",
        "\n",
        "`Concreteness` can increase a message's persuasion. The concreteness can be measured by:\n",
        "- the use of `article` (e.g., a, an, and the), \n",
        "- `adpositions` (e.g., in, at, of, on, etc), and\n",
        "- `quantifiers`, i.e., adjectives before nouns.\n",
        "\n",
        "\n",
        "Define a function `compute_concreteness(tagged_sent)` as follows:\n",
        "- Input argument is `tagged_sent`, a list with (token, pos_tag) tuples as shown above.\n",
        "- Find the three types of tokens: `articles`, `adposition`, and `quantifiers`.\n",
        "- Compute `concereness` score as:  `(the sum of the counts of the three types of tokens)/(total non-punctuation tokens)`.\n",
        "- return the concreteness score, articles, adposition, and quantifiers lists.\n",
        "\n",
        "\n",
        "Find the most concrete and the least concrete sentences from the article. \n",
        "\n",
        "\n",
        "Reference: Peer to Peer Lending: The Relationship Between Language Features, Trustworthiness, and Persuasion Success, https://socialmedialab.sites.stanford.edu/sites/g/files/sbiybj22976/files/media/file/larrimore-jacr-peer-to-peer.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FYsclg6IQ1A"
      },
      "outputs": [],
      "source": [
        "def compute_concreteness(tagged_sent):\n",
        "\n",
        "  articles = [i for i in tagged_sent if(i[1]=='DET')]\n",
        "  adpositions = [i for i in tagged_sent if(i[1]=='ADP')]\n",
        "  quantifiers = [i for i in tagged_sent if(i[1]=='ADJ')]\n",
        "  nonpunc = [i for i in tagged_sent if(i[1]!='PUNCT')]\n",
        "\n",
        "  concreteness = (len(articles)+len(adpositions)+len(quantifiers))/len(nonpunc)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "  return concreteness, articles, adpositions,quantifiers\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs9z-YUFIQ1A"
      },
      "outputs": [],
      "source": [
        "# tokenize with pos tag, without change the text much\n",
        "\n",
        "sents, tokenized_sents = preprocess(text, lemmatized = False, pos_tag = True, \n",
        "                                    remove_stopword=False, remove_punctuation = False, \n",
        "                                    lower_case = False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with one sentence\n",
        "\n",
        "idx = 1\n",
        "x = tokenized_sents[idx]\n",
        "concreteness, articles, adpositions,quantifier = compute_concreteness(x)\n",
        "sents[idx]\n",
        "concreteness, articles, adpositions,quantifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "ZjqmnntPEEwI",
        "outputId": "b9622653-ae53-41b4-c2c6-21c4a55c7ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Until recently, the conventional wisdom was that while AI was better than humans at data-driven decision making tasks, it was still inferior to humans for cognitive and creative ones.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.36666666666666664,\n",
              " [('the', 'DET')],\n",
              " [('Until', 'ADP'),\n",
              "  ('than', 'ADP'),\n",
              "  ('at', 'ADP'),\n",
              "  ('to', 'ADP'),\n",
              "  ('for', 'ADP')],\n",
              " [('conventional', 'ADJ'),\n",
              "  ('better', 'ADJ'),\n",
              "  ('inferior', 'ADJ'),\n",
              "  ('cognitive', 'ADJ'),\n",
              "  ('creative', 'ADJ')])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the most concrete and the least concrete sentences from the article\n",
        "\n",
        "concrete = [compute_concreteness(x)[0] for x in tokenized_sents]\n",
        "max_id = np.argmax(np.array(concrete))\n",
        "min_id = np.argmin(np.array(concrete))\n",
        "print (f\"The most concerete sentence:  {sents[max_id]}, {concrete[max_id]:.3f}\\n\")\n",
        "print (f\"The least concerete sentence:  {sents[min_id]}, {concrete[min_id]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q7C2heAFGpV",
        "outputId": "b15d75f6-795f-40a9-f522-11c5b73cecaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most concerete sentence:  Through a combination of your data assets and open datasets, train a model for the needs of specific sectors or divisions., 0.429\n",
            "\n",
            "The least concerete sentence:  Organizations should begin preparing now not only to capitalize on transformative AI, but to do their part to avoid undesirable futures and ensure that advanced AI is used to equitably benefit society.. Language-Based AI Tools Are Here to Stay., 0.100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFBMtpV7-4KM"
      },
      "source": [
        "### Q2.3. Generate TF-IDF representations for sentences (1 point,  0.5 point for use_idf option, 0.5 point for overall)\n",
        "\n",
        "Define a function `compute_tf_idf(sents, use_idf)` as follows: \n",
        "\n",
        "\n",
        "- Take the following two inputs:\n",
        "    - `sents`: tokenized sentences returned from Q2.1. These sentences form a corpus for you to calculate `TF-IDF` vectors.\n",
        "    - `use_idf`: if this option is true, return smoothed normalized `TF_IDF` vectors for all sentences; otherwise, just return normalized `TF` vector for each sentence.\n",
        "    \n",
        "    \n",
        "- Calculate `TF-IDF` vectors as shown in the lecture notes (Hint: you can slightly modify code segment 7.5 in NLP Lecture Notes (II) for this task)\n",
        "\n",
        "- Return the `TF-IDF` vectors  if `use_idf` is True.  Return the `TF` vectors if `use_idf` is False."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# library for normalization\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# numpy is the package for matrix caculation\n",
        "import numpy as np  \n",
        "import nltk, re, string\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "IsWvFgiIGk2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-19T07:27:56.846403Z",
          "start_time": "2021-10-19T07:27:56.837402Z"
        },
        "id": "pq9vq7uP-4KM"
      },
      "outputs": [],
      "source": [
        "def compute_tf_idf(sents, use_idf = True, min_df = 1):\n",
        "\n",
        "  # Step 1. get tokens of each document as list\n",
        "\n",
        "  def get_doc_tokens(doc):\n",
        "    # tokens=[token.strip() \\\n",
        "    #         for token in nltk.word_tokenize(doc.lower()) \\\n",
        "    #         if token.strip() not in stop_words and\\\n",
        "    #            token.strip() not in string.punctuation]\n",
        "    # sents, tokenized_sents = preprocess(text)\n",
        "    # tokens = sum(sents, [])\n",
        "    # create token count dictionary\n",
        "    token_count=nltk.FreqDist(doc)\n",
        "    return token_count\n",
        "\n",
        "  # step 2. process all documents to \n",
        "  # a dictionary of dictionaries\n",
        "  docs_tokens={idx:get_doc_tokens(doc) \\\n",
        "              for idx,doc in enumerate(sents)}\n",
        "\n",
        "  # step 3. get document-term matrix\n",
        "  dtm=pd.DataFrame.from_dict(docs_tokens, \\\n",
        "                           orient=\"index\" \n",
        "  )\n",
        "  dtm=dtm.fillna(0)\n",
        "  # sort by index (i.e. doc id)\n",
        "  dtm = dtm.sort_index(axis = 0)\n",
        "\n",
        "\n",
        "  # step 4. get normalized term frequency (tf) matrix\n",
        "  dtm2=dtm.values\n",
        "  doc_len=dtm2.sum(axis=1)\n",
        "  tf=np.divide(dtm2, doc_len[:,None])  \n",
        "  tf_idf = tf\n",
        "\n",
        "  if(use_idf==True):\n",
        "    # step 5. get idf\n",
        "    df=np.where(dtm2>0,1,0)\n",
        "    idf=np.log(np.divide(len(sents), \\\n",
        "            np.sum(df, axis=0)))+1\n",
        "    smoothed_idf=np.log(np.divide(len(sents)+1, np.sum(df, axis=0)+1))+1\n",
        "\n",
        "    # step 6. get tf-idf\n",
        "    s=tf*idf\n",
        "    # by default normalize by row\n",
        "    tf_idf=normalize(tf*idf) \n",
        "    smoothed_tf_idf=normalize(tf*smoothed_idf)\n",
        "    tf_idf = smoothed_tf_idf\n",
        "            \n",
        "  return tf_idf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sents, tokenized_sents = preprocess(text)\n",
        "tf_idf = compute_tf_idf(tokenized_sents, use_idf = True)\n",
        "\n",
        "# show shape of TF-IDF\n",
        "tf_idf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4GJWaJPTPQP",
        "outputId": "4c277108-65b9-44f2-b3c4-65ee110e1cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67, 515)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEYffajS-4KN"
      },
      "source": [
        "### Q2.4. Identify key sentences as summary \n",
        "\n",
        "`2 points, 0.5 point for steps 1-3 each, 0.5 for overall logic. Due ot different packages used, the output summary sentences may not be the same as the sample output. If different, please check the code to check if the coding logic is correct`\n",
        "\n",
        "The basic idea is that, in a coherence article, all sentences should center around some key ideas. If we can identify a subset of sentences, denoted as $S_{key}$, which precisely capture the key ideas,  then $S_{key}$ can be used as a summary. Moreover, $S_{key}$ should have high similarity to all the other sentences on average, because all sentences are centered around the key ideas contained in $S_{key}$. Therefore, we can identify whether a sentence belongs to $S_{key}$ by its similarity to all the other sentences.\n",
        "\n",
        "\n",
        "Define a function `get_summary(tf_idf, sents, topN = 5)`  as follows:\n",
        "\n",
        "- This function takes three inputs:\n",
        "    - `tf_idf`: the TF-IDF vectors of all the sentences in a document\n",
        "    - `sents`: the original sentences corresponding to the TF-IDF vectors\n",
        "    - `topN`: the top N sentences in the generated summary\n",
        "\n",
        "- Steps:\n",
        "    1. Calculate the cosine similarity for every pair of TF-IDF vectors (0.5 point)\n",
        "    1. For each sentence, calculate its average similarity to all the others (0.5 point)\n",
        "    1. Select the sentences with the `topN` largest average similarity (0.5 point)\n",
        "    1. Print the `topN` sentences index\n",
        "    1. Return these sentences as the summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# package to calculate distance\n",
        "from sklearn.metrics import pairwise_distances"
      ],
      "metadata": {
        "id": "8Xe24PNxgB7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-19T07:29:47.949165Z",
          "start_time": "2021-10-19T07:29:47.945086Z"
        },
        "id": "gDqgTk8c-4KO"
      },
      "outputs": [],
      "source": [
        "def get_summary(tf_idf, sents, topN = 5):\n",
        "    \n",
        "  # calculate cosince distance of every pair of documents \n",
        "  # similarity is 1-distance\n",
        "  similarity=1-pairwise_distances(tf_idf, metric = 'cosine')\n",
        "  avg_similarity = np.sum(similarity, axis=1)/len(similarity)\n",
        "  id = np.argsort(avg_similarity)[::-1][0:topN]\n",
        "  summary =[]\n",
        "  print(\"topN sentences index: \",id)\n",
        "  for i in id:\n",
        "    summary.append(sents[i])\n",
        "    \n",
        "\n",
        "\n",
        "  return summary "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  put everything together and test with different options\n",
        "\n",
        "sents, tokenized_sents = preprocess(text)\n",
        "tf_idf = compute_tf_idf(tokenized_sents, use_idf = True)\n",
        "summary = get_summary(tf_idf, sents, topN = 5)\n",
        "print(\"\\nSummary: \")\n",
        "for sent in summary:\n",
        "    print(sent,\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFOrdvo0j05N",
        "outputId": "8a300a6d-6ea2-45f6-8dfa-63efc8e848f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topN sentences index:  [45 63 40 14  9]\n",
            "\n",
            "Summary: \n",
            "Begin incorporating new language-based AI tools for a variety of tasks to better understand their capabilities.. \n",
            "\n",
            "Powerful generalizable language-based AI tools like Elicit are here, and they are just the tip of the iceberg; multimodal foundation model-based tools are poised to transform business in ways that are still difficult to predict. \n",
            "\n",
            "Understand how you might leverage AI-based language technologies to make better decisions or reorganize your skilled labor.. Language-based AI won’t replace jobs, but it will automate many tasks, even for decision makers. \n",
            "\n",
            "This transformative capability was already expected to change the nature of how programmers do their jobs, but models continue to improve — the latest from Google’s DeepMind AI lab, for example, demonstrates the critical thinking and logic skills necessary to outperform most humans in programming competitions.. Models like GPT-3 are considered to be foundation models — an emerging AI research area — which also work for other types of data such as images and video. \n",
            "\n",
            "NLP practitioners call tools like this “language models,” and they can be used for simple analytics tasks, such as classifying documents and analyzing the sentiment in blocks of text, as well as more advanced tasks, such as answering questions and summarizing reports. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ypAfhMGIQ1C",
        "outputId": "9c4b1d9f-277b-4766-e02d-f2f5853eef25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topN sentences index:  [54 16 25 44 14]\n",
            "Remember that while current AI might not be poised to replace managers, managers who understand AI are poised to replace managers who don’t.. Do not underestimate the transformative potential of AI.. Large foundation models like GPT-3 exhibit abilities to generalize to a large number of tasks without any task-specific training. \n",
            "\n",
            "Due to their potential to transform the nature of cognitive work, economists expect that foundation models may affect every part of the economy and could lead to increases in economic growth similar to the industrial revolution.. A Language-Based AI Research Assistant. \n",
            "\n",
            "I am also beginning to integrate brainstorming tasks into my work as well, and my experience with these tools has inspired my latest research, which seeks to utilize foundation models for supporting strategic planning.. How Can Organizations Prepare for the Future?. \n",
            "\n",
            "This may not be true for all software developers, but it has significant implications for tasks like data processing and web development.. \n",
            "\n",
            "This transformative capability was already expected to change the nature of how programmers do their jobs, but models continue to improve — the latest from Google’s DeepMind AI lab, for example, demonstrates the critical thinking and logic skills necessary to outperform most humans in programming competitions.. Models like GPT-3 are considered to be foundation models — an emerging AI research area — which also work for other types of data such as images and video. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# test with the option lemmatized=False, remove_stopword=False\n",
        "\n",
        "sents, tokenized_sents = preprocess(text, lemmatized=False, remove_stopword=False, remove_punctuation = True )\n",
        "tf_idf = compute_tf_idf(tokenized_sents, use_idf = True)\n",
        "summary = get_summary(tf_idf, sents, topN = 5)\n",
        "for sent in summary:\n",
        "   print(sent,\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y63RvdLiIQ1C",
        "outputId": "5e1bf258-5d81-4d08-bd19-bfa2b6419ac6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topN sentences index:  [63 45 40  9 14]\n",
            "Powerful generalizable language-based AI tools like Elicit are here, and they are just the tip of the iceberg; multimodal foundation model-based tools are poised to transform business in ways that are still difficult to predict. \n",
            "\n",
            "Begin incorporating new language-based AI tools for a variety of tasks to better understand their capabilities.. \n",
            "\n",
            "Understand how you might leverage AI-based language technologies to make better decisions or reorganize your skilled labor.. Language-based AI won’t replace jobs, but it will automate many tasks, even for decision makers. \n",
            "\n",
            "NLP practitioners call tools like this “language models,” and they can be used for simple analytics tasks, such as classifying documents and analyzing the sentiment in blocks of text, as well as more advanced tasks, such as answering questions and summarizing reports. \n",
            "\n",
            "This transformative capability was already expected to change the nature of how programmers do their jobs, but models continue to improve — the latest from Google’s DeepMind AI lab, for example, demonstrates the critical thinking and logic skills necessary to outperform most humans in programming competitions.. Models like GPT-3 are considered to be foundation models — an emerging AI research area — which also work for other types of data such as images and video. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# test with the option use_idf = False\n",
        "\n",
        "sents, tokenized_sents = preprocess(text)\n",
        "tf_idf = compute_tf_idf(tokenized_sents, use_idf = False)\n",
        "summary = get_summary(tf_idf, sents, topN = 5)\n",
        "for sent in summary:\n",
        "   print(sent,\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l0bXLQG-4KO"
      },
      "source": [
        "### Q2.5. Analysis (1 point, 0.5 point for Q1, and 0.5 for all the others)\n",
        "\n",
        "- Do you think the way to quantify concreteness makes sense? Any other thoughts to measure concreteness or abstractness? Share your ideas in pdf or markdown.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Do you think this method is able to generate a good summary? Any pros or cons have you observed? (0.5 point)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Do these options `lemmatized, remove_stopword, remove_punctuation, use_idf` matter? \n",
        "- Why do you think these options matter or do not matter? \n",
        "- If these options matter, what are the best values for these options?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis:\n",
        "\n",
        "We are calculating the proportion of definite words in a sentence by excluding the punctuations. This is a good way to find concreteness of a sentence. Other measures to find concreteness is to find rarity of adjectives in the whole document which adds up to the abstractness of that sentence.\n",
        "\n",
        "Similarity generates good summary. The main disadvantage of finding simalarity is few sentences which are important might be excluded from the similarity list.\n",
        "\n",
        "Yes these options matter because: \n",
        "\n",
        "- lemmatization is to reduce forms of a word to a common base form. The main advantage of lemmatization is that it takes into consideration the context of the word to determine which is the intended meaning the user is looking for \n",
        "- if we remove the words that are very commonly used in a given language, we can focus on the important words instead\n",
        "- punctuations are not of much of importance while generating summary. Therefore it's existence in data just contributes to the noise and so it should be removed\n",
        "- IDF can quantify the importance or relevance of string representations in a document amongst a collection of documents. TF-IDF weights words based on relevance, one can use this technique to determine that the words with the highest relevance are the most important. This can be used to help summarize articles more efficiently or to simply determine keywords for a document.\n",
        "\n",
        "So, the best values of these options is to give 'TRUE'"
      ],
      "metadata": {
        "id": "T2RYutMJLaUc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIyY8lIr-4KP"
      },
      "source": [
        "## Q2.5. (Bonus 1 point). \n",
        "\n",
        "`While the idea is proposed, it must be implemented. `\n",
        "\n",
        "\n",
        "- Can you think a way to improve this extractive summary method? Explain the method you propose for improvement,  implement it, use it to generate a new summary, and demonstrate what is improved in the new summary.\n",
        "\n",
        "**Sample Answer**\n",
        "\n",
        "\n",
        "A: If an article have sentences repeats themselves and if one of the repeated sentences is selected, the other will be selected too. It is hard to ensure the diverity of the sentences in the summary.  To ensure diversity, tor example, this algorithm can be improved using **max-min** method: \n",
        "1. Select top 10 (or more) sentences as before as candidates. \n",
        "1. Add top 1 from the candidates into the summary, \n",
        "1. gradually add other sentences such that each of them is **least similar** to those aleady added. \n",
        "An implementation is provided. See if it's better!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Or, you can research on some other extractive summary methods and implement one here. Compare it with the one you implemented in Q2.1-Q2.3 and show pros and cons of each method.\n",
        "\n",
        "*Another alogithm can be selecting the sentences which have the largest total word tf-idf scores. For implementation, see https://towardsdatascience.com/text-summarization-using-tf-idf-e64a0644ace3*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn_e2luHIQ1D"
      },
      "outputs": [],
      "source": [
        "def get_summary_with_diversity(tf_idf, sents, topN = 5):\n",
        "  sent_score = np.sum(tf_idf,axis=1)\n",
        "  avg_sent_score = sum(sent_score)/len(sent_score)\n",
        "  d = {}\n",
        "  for i,k in enumerate(sent_score):\n",
        "    if(sent_score[i]>avg_sent_score):\n",
        "      d[i] = k\n",
        "  # summary = np.argsort()[::-1][:topN]\n",
        "  d = dict(sorted(d.items(), key = lambda x:x[1], reverse=True))\n",
        "  ids = d.keys()\n",
        "  summary = []\n",
        "  for i in d.keys():\n",
        "    summary.append(sents[i])\n",
        "  summary = summary[:topN]\n",
        "  \n",
        "  return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BYqJe3eIQ1D",
        "outputId": "8648e094-77a4-4ac5-e452-954434dcf76e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This transformative capability was already expected to change the nature of how programmers do their jobs, but models continue to improve — the latest from Google’s DeepMind AI lab, for example, demonstrates the critical thinking and logic skills necessary to outperform most humans in programming competitions.. Models like GPT-3 are considered to be foundation models — an emerging AI research area — which also work for other types of data such as images and video. \n",
            "\n",
            "Tasks like data labeling and summarization are still rough around the edges, with noisy results and spotty accuracy, but research from Ought and research from OpenAI shows promise for the future.. For example, the rephrase task is useful for writing, but the lack of integration with word processing apps renders it impractical for now. \n",
            "\n",
            "The latest version, called InstructGPT, has been fine-tuned by humans to generate responses that are much better aligned with human values and user intentions, and Google’s latest model shows further impressive breakthroughs on language and reasoning.. For businesses, the three areas where GPT-3 has appeared most promising are writing, coding, and discipline-specific reasoning. \n",
            "\n",
            "Due to their potential to transform the nature of cognitive work, economists expect that foundation models may affect every part of the economy and could lead to increases in economic growth similar to the industrial revolution.. A Language-Based AI Research Assistant. \n",
            "\n",
            "Language models are already reshaping traditional text analytics, but GPT-3 was an especially pivotal language model because, at 10x larger than any previous model upon release, it was the first large language model, which enabled it to perform even more advanced tasks like programming and solving high school–level math problems. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "tf_idf = compute_tf_idf(tokenized_sents, use_idf = True)\n",
        "summary = get_summary_with_diversity(tf_idf, sents, topN = 5)\n",
        "\n",
        "for sent in summary:\n",
        "    print(sent,\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### New method for summary extraction:\n",
        "\n",
        "- We have the TF-IDF score for each word from compute_tf_idf() with use_idf=TRUE. \n",
        "- Now, using these scores, I have found the score for each document (sentence). The TF-IDF score of the words in each sentence are summed up to give us the sentence score i.e. sent_score. \n",
        "- Now, I'm calculating the threshold value. Threshold is the average value of the scores of the sentences i.e. sum(sent_score)/len(sent_score) \n",
        "- Finally, summary is generated by extracting the sentences having scores greater than the threshold value.\n",
        "\n",
        "Instead of using similarity to get the summary, directly getting the scores from TF-IDF matrix considers all the sentences which might be left out while checking for similarity. So, this gives the general summary which can be considered better than similaity method."
      ],
      "metadata": {
        "id": "EasnmB2E9HYg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FVIKOIRE8CbS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}